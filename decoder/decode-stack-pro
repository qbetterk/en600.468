#!/usr/bin/env python
import optparse
import time
import sys
import models
from collections import namedtuple
import numpy as np
optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/input", help="File containing sentences to translate (default=data/input)")
optparser.add_option("-t", "--translation-model", dest="tm", default="data/tm", help="File containing translation model (default=data/tm)")
optparser.add_option("-l", "--language-model", dest="lm", default="data/lm", help="File containing ARPA-format language model (default=data/lm)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to decode (default=no limit)")
optparser.add_option("-k", "--translations-per-phrase", dest="k", default=10, type="int", help="Limit on number of translations to consider per phrase (default=1)")
optparser.add_option("-s", "--stack-size", dest="s", default=10000, type="int", help="Maximum stack size (default=1)")
optparser.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,  help="Verbose mode (default=off)")
optparser.add_option("-d", "--distance", dest="distance", default=8, type=int,  help="maximum distance for reordering")
optparser.add_option("-a", "--alpha", dest="alpha", default=1, type=int,  help="maximum distance for reordering")

opts = optparser.parse_args()[0]

tm = models.TM(opts.tm, opts.k)
tm1 = models.TM(opts.tm, opts.k)  # for computing future cost
lm = models.LM(opts.lm)
french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

# tm should translate unknown words as-is with probability 1
for word in set(sum(french,())):
    if (word,) not in tm:
        tm[(word,)] = [models.phrase(word, 0.0)]
        tm1[(word,)] = [models.phrase(word, 0.0)]

def extract_english(h): 
    return "" if h.predecessor is None else "%s%s " % (extract_english(h.predecessor), h.phrase.english)

sys.stderr.write("Decoding %s...\n" % (opts.input,))
for num_f, f in enumerate(french):
    sys.stderr.write("Sentence {} ...\n".format(num_f))

  # # computing future cost table
    fcost = [[0] for _ in range(len(f))]
    for i in range(len(f)):
        fcost[i].append(tm1[f[i:i+1]][0].logprob)
        for j in range(i+1, len(f)):
            fcost[i].append(fcost[i][-1]  + tm[f[j:j+1]][0].logprob)
            for k in range(i,j):
                if f[k:j+1] in tm1:
                    if fcost[i][k-i] + tm1[f[k:j+1]][0].logprob > fcost[i][j-i+1]:
                        fcost[i][j-i+1] = fcost[i][k-i] + tm1[f[k:j+1]][0].logprob
        fcost[i].remove(fcost[i][0])
        fcost.append([0])

    hypothesis = namedtuple("hypothesis", "logprob, lm_state, end_index, predecessor, phrase, coverage")
    initial_hypothesis = hypothesis(0.0, lm.begin(), 0, None, None, np.zeros(len(f)))
    stacks = [{} for _ in range(len(f))] + [{}]
    # inital stack
    stacks[0][(lm.begin(), 0)] = initial_hypothesis
    for i, stack in enumerate(stacks[:-1]):
        for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:opts.s]: # prune
            previous_index = h.end_index
            # f_s_min = max(0, previous_index - opts.distance)
            # f_s_max = min(len(f), previous_index + opts.distance)
            f_s_min = 0
            f_s_max = len(f)
            for f_s in xrange(f_s_min, f_s_max):
                for f_e in xrange(f_s + 1, len(f) + 1):
                    #if 0 in h.coverage[:max(0, f_e - opts.distance)]:
                    #    continue
                    if 1 not in h.coverage[f_s:f_e]:
                        if f[f_s:f_e] in tm:
                            for phrase in tm[f[f_s:f_e]]:
                                # recordering cost
                                reorder_score = np.log(opts.alpha)* np.abs(previous_index - f_s)
                                # reorder_score = 0
                                # previous score and current tm score
                                logprob = h.logprob + phrase.logprob + reorder_score
                                # previous words
                                lm_state = h.lm_state
                                # current language model score
                                for word in phrase.english.split():
                                    (lm_state, word_logprob) = lm.score(lm_state, word)
                                    logprob +=  word_logprob
                                
                                coverage = np.copy(h.coverage)
                                coverage[f_s:f_e] = 1

                                m = f_s; n = f_e - 1;
                                while m >= 0 and h.coverage[m] == 0:
                                    m -= 1
                                while n <= len(f) - 1 and h.coverage[n] == 0:
                                    n += 1
                                if m != f_s - 1:
                                    logprob += fcost[m+1][f_s-m-2]
                                if n != f_e:
                                    logprob += fcost[f_e][n-f_e-1]

                                logprob -= fcost[m+1][n-m-2]

                                
                                logprob += lm.end(lm_state) if f_e == len(f) else 0.0

                                new_hypothesis = hypothesis(logprob, lm_state, f_e, h, phrase, coverage)
                                
                                target = i + f_e - f_s

                                if (lm_state, previous_index) not in stacks[target] or stacks[target][(lm_state, previous_index)].logprob < logprob:
                                    stacks[target][(lm_state, previous_index)] = new_hypothesis
                                        #time.sleep(2)
        #for value in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:opts.s]:
        #    print value.coverage
        #    print extract_english(value),value.logprob
        #raw_input('===================')
    winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
    print extract_english(winner)

    if opts.verbose:
        def extract_tm_logprob(h):
            return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
        tm_logprob = extract_tm_logprob(winner)
        sys.stderr.write("LM = %f, TM = %f, Total = %f\n" % 
                (winner.logprob - tm_logprob, tm_logprob, winner.logprob))
